# Proximal Policy Optimization (PPO)
Part of the **Deep Reinforcement Learning Algorithm Suite** (DQN â†’ A3C â†’ PPO â†’ SAC)

This folder contains a full PPO implementation supporting:
- Continuous action spaces (e.g., CarRacing-v2)
- CNN Actorâ€“Critic network
- Gaussian stochastic policy
- Generalized Advantage Estimation (GAE)
- Multi-epoch minibatch PPO updates
- Rollout collection with bootstrap values
- Logging, evaluation, and checkpointing

---

##  Folder Structure

ppo/
â”‚â”€â”€ policy.py          # CNN Actorâ€“Critic (mean, std, value)
â”‚â”€â”€ agent.py           # PPOAgent: rollout buffer, GAE, PPO update
â”‚â”€â”€ train_ppo.py       # Main training loop
â”‚â”€â”€ README.md          # This file

---

## Shared modules:
core/
â”‚â”€â”€ wrappers.py        # preprocess (resize, frame-stack, normalize)
â”‚â”€â”€ logger.py          # TensorBoard + PNG export
â”‚â”€â”€ utils.py           # seed, device helpers

---
---

## ðŸ§  Algorithm Overview

**PPO** is an on-policy actorâ€“critic method that maintains stability using:
- **Clipped objective**  
- **GAE advantages**  
- **Entropy regularization**  
- **Multiple epochs per rollout**

### Clipped Surrogate Objective
\[
L = \min \left( r_t A_t,\; \text{clip}(r_t, 1-\epsilon, 1+\epsilon) A_t \right)
\]

### GAE Advantages
\[
A_t = \sum_k (\gamma \lambda)^k \delta_{t+k}
\]

---

##  Network (policy.py)

The Actorâ€“Critic network contains:
- 3-layer CNN encoder  
- Fully connected latent layer  
- **Actor head** â†’ Gaussian `mean` + learned `log_std`  
- **Critic head** â†’ scalar state value  
- Orthogonal initialization  
- Supports CHW image input  

---

##  PPOAgent (agent.py)

Handles:
- Rollout storage (`states, actions, logprobs, rewards, dones, values`)
- GAE advantage computation
- PPO update with:
  - Ratio and clip objective  
  - Value loss  
  - Entropy bonus  
  - Gradient clipping  
- Old policy sync after updates
- Continuous action sampling with `Independent(Normal)`

---

##  Training Loop (train_ppo.py)

The training loop:
1. Collects `buffer_size` timesteps  
2. Computes bootstrap value for last state  
3. Runs `agent.ppo_update()`  
4. Logs progress and optionally renders  
5. Saves checkpoints if enabled  

Uses Gymnasiumâ€™s step API:
obs, reward, terminated, truncated, info
done = terminated or truncated

---

##  Usage

### Basic training:
```bash
python ppo/train_ppo.py --env CarRacing-v2 --total_timesteps 200000
python ppo/train_ppo.py --render
python ppo/train_ppo.py --buffer_size 4096 --mini_batch_size 128